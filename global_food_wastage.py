#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
import joblib
import streamlit as st

# í°íŠ¸ì§€ì •
plt.rcParams['font.family'] = 'Malgun Gothic'

# ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ì§€ì •
plt.rcParams['axes.unicode_minus'] = False

# ìˆ«ìê°€ ì§€ìˆ˜í‘œí˜„ì‹ìœ¼ë¡œ ë‚˜ì˜¬ ë•Œ ì§€ì •
pd.options.display.float_format = '{:.2f}'.format
# ë°ì´í„° ë¡œë“œ
data = pd.read_csv('dataset/global_food_wastage_dataset.csv')
# ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸
data.info() # ë°ì´í„° íƒ€ì…, ê²°ì¸¡ì¹˜ í™•ì¸
data.head()  # ê¸°ë³¸ í†µê³„ê°’ (í‰ê· , í‘œì¤€í¸ì°¨ ë“±)



# In[36]:


# ê²°ì¸¡ì¹˜ í™•ì¸
data.isnull().sum()


# In[ ]:


# íŠ¹ì„±(X)ê³¼ íƒ€ê²Ÿ(y) ë¶„ë¦¬
# ì˜¬ë°”ë¥¸ ë°©ë²•: ì—¬ëŸ¬ ê°œì˜ ì»¬ëŸ¼ì„ ë¦¬ìŠ¤íŠ¸([])ë¡œ ì„ íƒ
X = data[['Total Waste (Tons)', 'Avg Waste per Capita (Kg)', 'Population (Million)', 'Household Waste (%)']]
y = data['Economic Loss (Million $)']

print(X.head())  # X ë°ì´í„° í™•ì¸
print(y.head())  # y ë°ì´í„° í™•ì¸



# In[ ]:


# ë°ì´í„° ì „ì²˜ë¦¬: íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
# íŠ¹ì„±ë§ˆë‹¤ ë‹¨ìœ„ì™€ ê°’ì˜ í¬ê¸°ê°€ ë‹¤ë¥´ë¯€ë¡œ, í‘œì¤€í™”ê°€ í•„ìš”
# ë°ì´í„° í¬ê¸°ê°€ ë‹¤ë¥¼ ê²½ìš°, ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•˜ë©´ ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ê°€ ë¹¨ë¼ì§€ê³ , ì„±ëŠ¥ì´ í–¥ìƒë¨.
data.describe() # ê° ì—´(ì»¬ëŸ¼)ì˜ í‰ê· (mean), í‘œì¤€í¸ì°¨(std), ìµœì†Œê°’(min), ìµœëŒ€ê°’(max) ë“±ì„ í™•ì¸
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X) # StandardScaler()ë¡œ ë³€í™˜í•œ ë°ì´í„°ëŠ” NumPy ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜ë˜ë¯€ë¡œ, ë‹¤ì‹œ pandas DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
X_scaled = pd.DataFrame(X_scaled, columns=X.columns) # columns=X.columnsë¥¼ ì¶”ê°€í•˜ì—¬ ì›ë˜ Xì˜ ì»¬ëŸ¼ëª…ì„ ìœ ì§€í•©ë‹ˆë‹¤.
# ë³€í™˜ëœ ë°ì´í„° í™•ì¸
print(X_scaled.head())


# In[ ]:


# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
# Random Forest ëª¨ë¸ ìƒì„±
# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•´ GridSearchCV ì‚¬ìš©
param_grid = {
    'n_estimators': [50, 100, 200],  # íŠ¸ë¦¬ì˜ ê°œìˆ˜
    'max_depth': [4, 6, 8],       # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
    'min_samples_split': [2, 4],  # ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    'min_samples_leaf': [1, 2]    # ë¦¬í”„ ë…¸ë“œì— ìˆì–´ì•¼ í•˜ëŠ” ìµœì†Œ ìƒ˜í”Œ ìˆ˜
}


# In[ ]:


rf_model = RandomForestRegressor(random_state=42)  # Random Forest ëª¨ë¸ ì´ˆê¸°í™”
# GridSearchCVëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”(íŠœë‹)ë¥¼ ìœ„í•œ ë„êµ¬.
# ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„°(param_grid) ì¡°í•©ì„ ëª¨ë‘ í…ŒìŠ¤íŠ¸í•˜ì—¬ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ ì„ íƒ.
# **êµì°¨ ê²€ì¦(Cross Validation, CV)**ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ê³¼ì í•©ë˜ì§€ ì•Šë„ë¡ í‰ê°€.
# estimator=rf_model	í‰ê°€í•  ëª¨ë¸ (ëœë¤ í¬ë ˆìŠ¤íŠ¸)
# param_grid=param_grid	ì‹¤í—˜í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©
# cv=3	3-Fold êµì°¨ ê²€ì¦ ìˆ˜í–‰
# scoring='r2'	ëª¨ë¸ í‰ê°€ ê¸°ì¤€ â†’ ê²°ì •ê³„ìˆ˜(RÂ², R-squared) ì‚¬ìš©
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='r2')


# In[ ]:


# ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ í•˜ë‚˜ì”© ì ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµ
# ê°€ì¥ ë†’ì€ RÂ² Score(ì„±ëŠ¥)ë¥¼ ê¸°ë¡í•œ ëª¨ë¸ì„ best_estimator_ë¡œ ì €ì¥
grid_search.fit(X_train, y_train)


# In[ ]:


# ìµœì ì˜ íŒŒë¼ë¯¸í„° ì¶œë ¥
print("Best Parameters:", grid_search.best_params_)
# GridSearchCVì—ì„œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì€ í›„, ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì„ ì €ì¥
best_rf_model = grid_search.best_estimator_
# ëª¨ë¸ ì €ì¥
joblib.dump(best_rf_model, 'food_wastage_model.pkl')
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ (í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ìµœì íšŒëœ ìƒíƒœë¡œ ì˜ˆì¸¡ì´ ì´ë£¨ì–´ì§)
y_pred = best_rf_model.predict(X_test)


# In[ ]:


# ëª¨ë¸ ë¡œë”©ì„ ìºì‹±í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
@st.cache_resource
def load_model():
    return joblib.load("food_wastage_model.pkl")

model = load_model()
@st.cache_data
def predict_economic_loss(TotalWaste, AvgWasteperCapita, HouseholdWaste, Population):
    input_data = np.array([[TotalWaste, AvgWasteperCapita, HouseholdWaste, Population]])
    prediction = model.predict(input_data)[0]
    return prediction


# In[ ]:


# 3. Streamlit ì•±
st.title('ì‹ëŸ‰ íê¸°ë¡œ ì¸í•œ ê²½ì œì  ì†ì‹¤ ì˜ˆì¸¡ ì‹œìŠ¤í…œ')
st.write('Total Waste (Tons),Avg Waste per Capita, Household Waste, Population ê°’ì„ ì…ë ¥í•˜ì—¬ ê²½ì œì  ì†ì‹¤ì„ ì˜ˆì¸¡ì„ í•´ë³´ì„¸ìš”.')


# In[ ]:


# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸° (ì¡°ì •ëœ ë²”ìœ„ ì„¤ì •)
TotalWaste = st.slider(
    'Total Waste (Tons) (ì´ íê¸°ë¬¼ëŸ‰)', 
    min_value=1000, max_value=50000, value=20000, step=1000
)

AvgWasteperCapita = st.slider(
    'Avg Waste per Capita (Kg) (1ì¸ë‹¹ í‰ê·  íê¸°ë¬¼ëŸ‰)', 
    min_value=50.0, max_value=200.0, value=100.0, step=0.5
)

HouseholdWaste = st.slider(
    'Household Waste (%) (ê°€ì •ì—ì„œ ë°œìƒí•˜ëŠ” íê¸°ë¬¼ ë¹„ìœ¨)', 
    min_value=10, max_value=60, value=30, step=1
)

Population = st.slider(
    'Population (Million) (ì¸êµ¬ìˆ˜)', 
    min_value=50, max_value=1200, value=500, step=50
)


# In[ ]:


# ì˜ˆì¸¡ ë²„íŠ¼
if st.button('ì˜ˆì¸¡í•˜ê¸°'):
    prediction = predict_economic_loss(TotalWaste, AvgWasteperCapita, HouseholdWaste, Population)
    st.success(f'ğŸ“Œ ì˜ˆìƒ ê²½ì œì  ì†ì‹¤: {prediction:,.2f} (Million $)')

    


# In[ ]:


# ê²°ê³¼ ì‹œê°í™” ì‚°ì ë„ ê·¸ë˜í”„
# **ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€ ëª¨ë¸ì˜ ì‹¤ì œê°’(y_test)ê³¼ ì˜ˆì¸¡ê°’(y_pred)ì„ ë¹„êµ
# ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í–ˆëŠ”ì§€ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸
# í•˜ë‚˜ì˜ ì  = í•œ ê°œì˜ ë°ì´í„° ìƒ˜í”Œ
# ì ì´ y=x(45ë„ ëŒ€ê°ì„ )ì— ê°€ê¹Œìš¸ìˆ˜ë¡, ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì •í™•í•œ ê²ƒ.
# ì ë“¤ì´ 45ë„ ëŒ€ê°ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ìŒ!
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.xlabel('ì‹¤ì œê°’ (MEDV)')
plt.ylabel('ì˜ˆì¸¡ê°’ (Predicted MEDV)')
plt.title('ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ë¹„êµ')
plt.grid()
plt.show()


# In[1]:


import seaborn as sns
import matplotlib.pyplot as plt

# íŠ¹ì • ì»¬ëŸ¼ë§Œ ì„ íƒ
selected_columns = ['Total Waste (Tons)', 'Avg Waste per Capita (Kg)', 'Population (Million)', 'Household Waste (%)', 'Economic Loss (Million $)']
selected_data = data[selected_columns]  # ì„ íƒí•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ

# ë°ì´í„° ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ 
# ë°ì´í„°ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„
# ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„(Correlation)ë¥¼ ìˆ«ìë¡œë§Œ ë³´ëŠ” ê²ƒë³´ë‹¤ ì‹œê°ì ìœ¼ë¡œ í™•ì¸
# ì–´ë–¤ ë³€ìˆ˜ê°€ ì„œë¡œ ê°•í•œ ê´€ê³„(ì–‘ì˜ ìƒê´€ê´€ê³„ ë˜ëŠ” ìŒì˜ ìƒê´€ê´€ê³„)ë¥¼ ê°€ì§€ëŠ”ì§€ í•œëˆˆì— íŒŒì•…
# ìƒê´€ê´€ê³„ê°€ ë‚®ê±°ë‚˜ ì—†ëŠ” ë³€ìˆ˜ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ â†’ ë³€ìˆ˜ ì„ íƒì— ë„ì›€.
# ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ë³€ìˆ˜ëŠ” ì œê±°í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒ 
plt.figure(figsize=(8, 6))
sns.heatmap(selected_data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ')
plt.show()


# In[70]:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
# êµì°¨ ê²€ì¦ ìˆ˜í–‰
cv_scores = cross_val_score(best_rf_model, X_scaled, y, cv=5)


# In[66]:


# íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
feature_importance = pd.DataFrame({
    'íŠ¹ì„±': X.columns,
    'ì¤‘ìš”ë„': best_rf_model.feature_importances_
}).sort_values('ì¤‘ìš”ë„', ascending=False)


# In[71]:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
plt.figure(figsize=(10, 6))
sns.barplot(x='ì¤‘ìš”ë„', y='íŠ¹ì„±', data=feature_importance)
plt.title('íŠ¹ì„± ì¤‘ìš”ë„')
plt.show()


# In[48]:


# ëœë¤ í¬ë ˆìŠ¤íŠ¸ í‰ê°€
rf_mse = mean_squared_error(y_test, y_pred)
rf_r2 = r2_score(y_test, y_pred)
print(f"ëœë¤ í¬ë ˆìŠ¤íŠ¸ - MSE: {rf_mse:.2f}, R2: {rf_r2:.2f}")


# In[5]:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
import joblib
import streamlit as st
# 2. ë‹¤ë¥¸ ëª¨ë¸ ë¹„êµ
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42)
}
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results[name] = {"MSE": mse, "R2": r2}
    print(f"{name} - MSE: {mse:.2f}, R2: {r2:.2f}")


# In[52]:


# 3. ìµœì  ëª¨ë¸ ì„ íƒ ë° ì‹œê°í™”
results["Random Forest"] = {"MSE": rf_mse, "R2": rf_r2}
best_model = max(results, key=lambda x: results[x]['R2'])
print("\nìµœì  ëª¨ë¸:", best_model)


# In[ ]:


# í°íŠ¸ì§€ì •
plt.rcParams['font.family'] = 'Malgun Gothic'

# ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ì§€ì •
plt.rcParams['axes.unicode_minus'] = False

# ìˆ«ìê°€ ì§€ìˆ˜í‘œí˜„ì‹ìœ¼ë¡œ ë‚˜ì˜¬ ë•Œ ì§€ì •
pd.options.display.float_format = '{:.2f}'.format
# ê²°ê³¼ ì‹œê°í™”
r2_scores = [result["R2"] for result in results.values()]
model_names = list(results.keys())
plt.figure(figsize=(10, 6))
plt.bar(model_names, r2_scores, color='skyblue')
plt.xlabel("ëª¨ë¸")
plt.ylabel("R2 Score")
plt.title("ëª¨ë¸ë³„ R2 Score ë¹„êµ")
plt.grid(axis='y')
plt.show()


# In[6]:


# ê° ë³€ìˆ˜ì˜ ë¶„í¬ í™•ì¸í•˜ëŠ” íˆìŠ¤í† ê·¸ë¨
# ê° ë³€ìˆ˜ì˜ ë¶„í¬(Distribution)ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸(ì •ê·œ ë¶„í¬ì¸ì§€, ì™œê³¡ë˜ì—ˆëŠ”ì§€ ë“±)
selected_columns = ['Total Waste (Tons)', 'Avg Waste per Capita (Kg)', 'Population (Million)', 'Household Waste (%)','Economic Loss (Million $)']
data[selected_columns].hist(figsize=(8, 6), bins=30)

# ê·¸ë˜í”„ ì¶œë ¥
plt.suptitle("ë³€ìˆ˜ì˜ ë¶„í¬", fontsize=14)
plt.show()


# In[ ]:


# "ì´ íê¸°ë¬¼ëŸ‰(Total Waste (Tons))"ê³¼ "ê²½ì œì  ì†ì‹¤(Economic Loss (Million $))" ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ê·¸ë˜í”„
# íê¸°ë¬¼ëŸ‰(Total Waste (Tons))ì´ ì¦ê°€í• ìˆ˜ë¡ ê²½ì œì  ì†ì‹¤(Economic Loss)ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸
# Xì˜ íŠ¹ì • ë³€ìˆ˜ë¥¼ xì¶•ìœ¼ë¡œ ì„¤ì • (ì˜ˆ: ì´ íê¸°ë¬¼ëŸ‰)
x_axis = X['Total Waste (Tons)']

# ì„  ê·¸ë˜í”„
plt.figure(figsize=(8, 5))
plt.plot(x_axis, y, marker='o', linestyle='-', color='b', label='Economic Loss')

# ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
plt.xlabel('Total Waste (Tons)')  # Xì¶• ë¼ë²¨
plt.ylabel('Economic Loss (Million $)')  # Yì¶• ë¼ë²¨
plt.title('Total Waste vs Economic Loss')  # ê·¸ë˜í”„ ì œëª©
plt.legend()
plt.grid(True)  # ê²©ì í‘œì‹œ
plt.show()


# In[ ]:


from ydata_profiling import ProfileReport
# YData Profilingìœ¼ë¡œ ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë³´ê³ ì„œ ìƒì„±
profile = ProfileReport(data, title="global_food_wastage Profiling Report", explorative=True)

# ë³´ê³ ì„œë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥
output_file = 'report/global_food_wastage_profiling_report.html'
profile.to_file(output_file)
print(f"í”„ë¡œíŒŒì¼ë§ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")

